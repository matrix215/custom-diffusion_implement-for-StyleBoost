{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.get_device_name() # CUDA를 실행하고 있는 기기 이름을 나타낸다.\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "conda customt ::training 용으로 이 파일을 실행하면 된다.\n",
    "conda custom3 :: 정규화 이미지 용으로 써 python retrieve.py --class_prompt 'wooden pot' --class_data_dir real_reg/samples_wooden_pot --num_class_images 200를 실행하면 된다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkko2/anaconda3/envs/customt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
      "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
      "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
      "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
      "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
      "    \n",
      "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
      "    Setting a new token will erase the existing one.\n",
      "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
      "Token is valid (permission: write).\n",
      "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
      "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
      "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
      "\n",
      "git config --global credential.helper store\n",
      "\n",
      "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
      "Token has not been saved to git credential helper.\n",
      "Your token has been saved to /home/kkko2/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import interpreter_login\n",
    "\n",
    "interpreter_login()\n",
    "#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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "02/20/2024 15:43:41 - INFO - __main__ - Distributed environment: NO\n",
      "Num processes: 1\n",
      "Process index: 0\n",
      "Local process index: 0\n",
      "Device: cuda\n",
      "\n",
      "Mixed precision type: fp16\n",
      "\n",
      "You are using a model of type clip_text_model to instantiate a model of type . This is not supported for all configurations of models and can yield errors.\n",
      "{'thresholding', 'sample_max_value', 'dynamic_thresholding_ratio', 'clip_sample_range', 'timestep_spacing', 'prediction_type', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "{'num_attention_heads', 'use_linear_projection', 'dropout', 'addition_embed_type', 'only_cross_attention', 'time_cond_proj_dim', 'resnet_skip_time_act', 'upcast_attention', 'cross_attention_norm', 'class_embeddings_concat', 'transformer_layers_per_block', 'conv_out_kernel', 'dual_cross_attention', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'timestep_post_act', 'projection_class_embeddings_input_dim', 'conv_in_kernel', 'resnet_time_scale_shift', 'time_embedding_dim', 'mid_block_type', 'time_embedding_type', 'encoder_hid_dim', 'addition_embed_type_num_heads', 'time_embedding_act_fn', 'num_class_embeds', 'addition_time_embed_dim', 'encoder_hid_dim_type', 'class_embed_type', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "/home/kkko2/custom/train_custom_diffusion.py:188: DeprecationWarning: BILINEAR is deprecated and will be removed in Pillow 10 (2023-07-01). Use Resampling.BILINEAR instead.\n",
      "  self.interpolation = Image.BILINEAR\n",
      "02/20/2024 15:43:50 - INFO - __main__ - ***** Running training *****\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Num examples = 40\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Num batches each epoch = 40\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Num Epochs = 25\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Instantaneous batch size per device = 1\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Gradient Accumulation steps = 1\n",
      "02/20/2024 15:43:50 - INFO - __main__ -   Total optimization steps = 1000\n",
      "Steps:   0%|                                           | 0/1000 [00:00<?, ?it/s]/home/kkko2/anaconda3/envs/customt/lib/python3.10/site-packages/xformers/ops/fmha/flash.py:339: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  and inp.query.storage().data_ptr() == inp.key.storage().data_ptr()\n",
      "Steps:  25%|███▎         | 250/1000 [00:57<02:45,  4.54it/s, loss=2.57, lr=2e-5]02/20/2024 15:44:47 - INFO - accelerate.accelerator - Saving current state to jouner-ani-1000/checkpoint-250\n",
      "02/20/2024 15:44:47 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-250/pytorch_model.bin\n",
      "02/20/2024 15:44:48 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-250/pytorch_model_1.bin\n",
      "02/20/2024 15:44:49 - INFO - accelerate.checkpointing - Optimizer state saved in jouner-ani-1000/checkpoint-250/optimizer.bin\n",
      "02/20/2024 15:44:49 - INFO - accelerate.checkpointing - Scheduler state saved in jouner-ani-1000/checkpoint-250/scheduler.bin\n",
      "02/20/2024 15:44:49 - INFO - accelerate.checkpointing - Gradient scaler state saved in jouner-ani-1000/checkpoint-250/scaler.pt\n",
      "02/20/2024 15:44:49 - INFO - accelerate.checkpointing - Random states saved in jouner-ani-1000/checkpoint-250/random_states_0.pkl\n",
      "02/20/2024 15:44:49 - INFO - accelerate.checkpointing - Saving the state of AttnProcsLayers to jouner-ani-1000/checkpoint-250/custom_checkpoint_0.pkl\n",
      "02/20/2024 15:44:49 - INFO - __main__ - Saved state to jouner-ani-1000/checkpoint-250\n",
      "Steps:  50%|██████      | 500/1000 [01:54<01:47,  4.66it/s, loss=0.333, lr=2e-5]02/20/2024 15:45:45 - INFO - accelerate.accelerator - Saving current state to jouner-ani-1000/checkpoint-500\n",
      "02/20/2024 15:45:45 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-500/pytorch_model.bin\n",
      "02/20/2024 15:45:46 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-500/pytorch_model_1.bin\n",
      "02/20/2024 15:45:46 - INFO - accelerate.checkpointing - Optimizer state saved in jouner-ani-1000/checkpoint-500/optimizer.bin\n",
      "02/20/2024 15:45:46 - INFO - accelerate.checkpointing - Scheduler state saved in jouner-ani-1000/checkpoint-500/scheduler.bin\n",
      "02/20/2024 15:45:46 - INFO - accelerate.checkpointing - Gradient scaler state saved in jouner-ani-1000/checkpoint-500/scaler.pt\n",
      "02/20/2024 15:45:46 - INFO - accelerate.checkpointing - Random states saved in jouner-ani-1000/checkpoint-500/random_states_0.pkl\n",
      "02/20/2024 15:45:46 - INFO - accelerate.checkpointing - Saving the state of AttnProcsLayers to jouner-ani-1000/checkpoint-500/custom_checkpoint_0.pkl\n",
      "02/20/2024 15:45:46 - INFO - __main__ - Saved state to jouner-ani-1000/checkpoint-500\n",
      "Steps:  75%|████████▎  | 750/1000 [02:51<00:53,  4.69it/s, loss=0.0545, lr=2e-5]02/20/2024 15:46:42 - INFO - accelerate.accelerator - Saving current state to jouner-ani-1000/checkpoint-750\n",
      "02/20/2024 15:46:42 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-750/pytorch_model.bin\n",
      "02/20/2024 15:46:43 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-750/pytorch_model_1.bin\n",
      "02/20/2024 15:46:43 - INFO - accelerate.checkpointing - Optimizer state saved in jouner-ani-1000/checkpoint-750/optimizer.bin\n",
      "02/20/2024 15:46:43 - INFO - accelerate.checkpointing - Scheduler state saved in jouner-ani-1000/checkpoint-750/scheduler.bin\n",
      "02/20/2024 15:46:43 - INFO - accelerate.checkpointing - Gradient scaler state saved in jouner-ani-1000/checkpoint-750/scaler.pt\n",
      "02/20/2024 15:46:43 - INFO - accelerate.checkpointing - Random states saved in jouner-ani-1000/checkpoint-750/random_states_0.pkl\n",
      "02/20/2024 15:46:43 - INFO - accelerate.checkpointing - Saving the state of AttnProcsLayers to jouner-ani-1000/checkpoint-750/custom_checkpoint_0.pkl\n",
      "02/20/2024 15:46:43 - INFO - __main__ - Saved state to jouner-ani-1000/checkpoint-750\n",
      "Steps: 100%|████████████| 1000/1000 [03:50<00:00,  4.11it/s, loss=1.08, lr=2e-5]02/20/2024 15:47:40 - INFO - accelerate.accelerator - Saving current state to jouner-ani-1000/checkpoint-1000\n",
      "02/20/2024 15:47:40 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-1000/pytorch_model.bin\n",
      "02/20/2024 15:47:41 - INFO - accelerate.checkpointing - Model weights saved in jouner-ani-1000/checkpoint-1000/pytorch_model_1.bin\n",
      "02/20/2024 15:47:41 - INFO - accelerate.checkpointing - Optimizer state saved in jouner-ani-1000/checkpoint-1000/optimizer.bin\n",
      "02/20/2024 15:47:41 - INFO - accelerate.checkpointing - Scheduler state saved in jouner-ani-1000/checkpoint-1000/scheduler.bin\n",
      "02/20/2024 15:47:41 - INFO - accelerate.checkpointing - Gradient scaler state saved in jouner-ani-1000/checkpoint-1000/scaler.pt\n",
      "02/20/2024 15:47:41 - INFO - accelerate.checkpointing - Random states saved in jouner-ani-1000/checkpoint-1000/random_states_0.pkl\n",
      "02/20/2024 15:47:41 - INFO - accelerate.checkpointing - Saving the state of AttnProcsLayers to jouner-ani-1000/checkpoint-1000/custom_checkpoint_0.pkl\n",
      "02/20/2024 15:47:42 - INFO - __main__ - Saved state to jouner-ani-1000/checkpoint-1000\n",
      "Steps: 100%|███████████| 1000/1000 [03:51<00:00,  4.11it/s, loss=0.052, lr=2e-5]Model weights saved in jouner-ani-1000/pytorch_custom_diffusion_weights.bin\n",
      "02/20/2024 15:47:42 - INFO - __main__ - Saving embeddings\n",
      "{'requires_safety_checker'} was not found in config. Values will be initialized to default values.\n",
      "\n",
      "Loading pipeline components...:   0%|                     | 0/7 [00:00<?, ?it/s]\u001b[ALoaded feature_extractor as CLIPImageProcessor from `feature_extractor` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loaded safety_checker as StableDiffusionSafetyChecker from `safety_checker` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  29%|███▋         | 2/7 [00:00<00:02,  2.47it/s]\u001b[A{'num_attention_heads', 'use_linear_projection', 'dropout', 'addition_embed_type', 'only_cross_attention', 'time_cond_proj_dim', 'resnet_skip_time_act', 'upcast_attention', 'cross_attention_norm', 'class_embeddings_concat', 'transformer_layers_per_block', 'conv_out_kernel', 'dual_cross_attention', 'resnet_out_scale_factor', 'mid_block_only_cross_attention', 'timestep_post_act', 'projection_class_embeddings_input_dim', 'conv_in_kernel', 'resnet_time_scale_shift', 'time_embedding_dim', 'mid_block_type', 'time_embedding_type', 'encoder_hid_dim', 'addition_embed_type_num_heads', 'time_embedding_act_fn', 'num_class_embeds', 'addition_time_embed_dim', 'encoder_hid_dim_type', 'class_embed_type', 'attention_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded unet as UNet2DConditionModel from `unet` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  43%|█████▌       | 3/7 [00:02<00:03,  1.08it/s]\u001b[A{'timestep_spacing', 'prediction_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded scheduler as PNDMScheduler from `scheduler` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "Loaded tokenizer as CLIPTokenizer from `tokenizer` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "{'force_upcast', 'scaling_factor'} was not found in config. Values will be initialized to default values.\n",
      "Loaded vae as AutoencoderKL from `vae` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...:  86%|███████████▏ | 6/7 [00:02<00:00,  2.52it/s]\u001b[ALoaded text_encoder as CLIPTextModel from `text_encoder` subfolder of runwayml/stable-diffusion-v1-5.\n",
      "\n",
      "Loading pipeline components...: 100%|█████████████| 7/7 [00:03<00:00,  2.22it/s]\u001b[A\n",
      "{'solver_order', 'thresholding', 'sample_max_value', 'use_karras_sigmas', 'solver_type', 'prediction_type', 'dynamic_thresholding_ratio', 'timestep_spacing', 'lambda_min_clipped', 'algorithm_type', 'lower_order_final', 'variance_type'} was not found in config. Values will be initialized to default values.\n",
      "Loaded textual inversion embedding for <new1>.\n",
      "Loaded textual inversion embedding for <new2>.\n",
      "Steps: 100%|███████████| 1000/1000 [03:57<00:00,  4.21it/s, loss=0.052, lr=2e-5]\n"
     ]
    }
   ],
   "source": [
    "#multi\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= \"2\"\n",
    "\n",
    "MODEL_NAME=\"runwayml/stable-diffusion-v1-5\"\n",
    "OUTPUT_DIR=\"jouner-ani-1000\"\n",
    "\n",
    "!accelerate launch train_custom_diffusion.py \\\n",
    "  --pretrained_model_name_or_path=$MODEL_NAME  \\\n",
    "  --output_dir=$OUTPUT_DIR \\\n",
    "  --concepts_list=./concept_list.json \\\n",
    "  --with_prior_preservation --real_prior --prior_loss_weight=1.0 \\\n",
    "  --resolution=512  \\\n",
    "  --train_batch_size=1  \\\n",
    "  --learning_rate=1e-5  \\\n",
    "  --lr_warmup_steps=0 \\\n",
    "  --max_train_steps=1000 \\\n",
    "  --num_class_images=20 \\\n",
    "  --scale_lr --hflip  \\\n",
    "  --modifier_token \"<new1>+<new2>\" \\\n",
    "  --enable_xformers_memory_efficient_attention \\\n",
    "  --no_safe_serialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kkko2/anaconda3/envs/customt/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading pipeline components...:  71%|███████▏  | 5/7 [00:00<00:00,  5.65it/s]`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n",
      "Loading pipeline components...: 100%|██████████| 7/7 [00:01<00:00,  4.05it/s]\n",
      "100%|██████████| 30/30 [00:02<00:00, 10.75it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from diffusers import DiffusionPipeline\n",
    "\n",
    "\n",
    "pipe = DiffusionPipeline.from_pretrained(\n",
    "    \"CompVis/stable-diffusion-v1-4\", torch_dtype=torch.float16, use_safetensors=True\n",
    ").to(\"cuda\")\n",
    "pipe.unet.load_attn_procs(\"/home/kkko2/custom/jouner-ani-1000\", weight_name=\"pytorch_custom_diffusion_weights.bin\")\n",
    "pipe.load_textual_inversion(\"/home/kkko2/custom/jouner-ani-1000\", weight_name=\"<new1>.bin\")\n",
    "pipe.load_textual_inversion(\"/home/kkko2/custom/jouner-ani-1000\", weight_name=\"<new2>.bin\")\n",
    "\n",
    "prompt_id = dict()\n",
    "with open('prompt.txt','r') as f:\n",
    "        prompt_lst = f.readlines()  \n",
    "    \n",
    "        for i in range(615,616): \n",
    "    \n",
    "                prompt_id[str(i+1)] = prompt_lst[i].rstrip('\\n')\n",
    "\n",
    "for prmpt_id in prompt_id.keys():\n",
    "    prompt_id[prmpt_id] = '<new1> style, <new2> style, '+prompt_id[prmpt_id]\n",
    "    image = pipe(\n",
    "        prompt_id[prmpt_id],\n",
    "        num_inference_steps=30,\n",
    "        guidance_scale=6.0,\n",
    "        eta=1.0,\n",
    "    ).images[0]\n",
    "    image.save(\"/home/kkko2/custom/output_img/home/{}.png\".format(prmpt_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading real regularization images: 100%|███| 20/20 [00:09<00:00,  2.21it/s]\n"
     ]
    }
   ],
   "source": [
    "!python retrieve.py --class_prompt 'cat' --class_data_dir real_reg/samples_cat --num_class_images 20"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "customt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
